{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9074cd-aa65-43e5-9158-f19c7b1cc937",
   "metadata": {},
   "source": [
    "# Load and Preprocess the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8082b50f-c47e-4c35-9895-a382f539cba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 69600 images belonging to 1 classes.\n",
      "Found 17400 images belonging to 1 classes.\n",
      "Found 28 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Paths to your dataset\n",
    "train_dir = \"asl_alphabet_train/\"\n",
    "test_dir = \"asl_alphabet_test/\"\n",
    "\n",
    "# Define image size and batch size\n",
    "img_size = (64, 64) \n",
    "batch_size = 32\n",
    "\n",
    "# Data augmentation and preprocessing for training set\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,  # Normalize pixel values\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    validation_split=0.2 \n",
    ")\n",
    "\n",
    "# Data preprocessing for test set\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load and preprocess training data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,  # Ensure target_size is a tuple\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb', \n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Load and preprocess validation data\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=img_size,  # Ensure target_size is a tuple\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Load and preprocess test data\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=img_size,  # Ensure target_size is a tuple\n",
    "    batch_size=batch_size,\n",
    "    color_mode='rgb',\n",
    "    class_mode='categorical'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d96ce85-5c8f-4a44-842b-fbaf94b3e0e8",
   "metadata": {},
   "source": [
    "# Define the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1899ddd-a5c8-4d06-a80e-929c8dfd540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Input(shape=(64, 64, 3)),\n",
    "        Conv2D(32, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')  # Update this line\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Update this line\n",
    "    return model\n",
    "\n",
    "model = create_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f48be95-059b-4432-bcd9-0e3788d162e1",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "633f7cab-9e32-4d43-803c-fe21bd2d9ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suyash\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m300s\u001b[0m 136ms/step - accuracy: 0.9975 - loss: 0.0030 - val_accuracy: 1.0000 - val_loss: 1.5399e-20\n",
      "Epoch 2/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 5.8713e-19 - val_accuracy: 1.0000 - val_loss: 1.1101e-20\n",
      "Epoch 3/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 6.8895e-22 - val_accuracy: 1.0000 - val_loss: 1.0221e-20\n",
      "Epoch 4/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 2.5793e-21 - val_accuracy: 1.0000 - val_loss: 4.5961e-21\n",
      "Epoch 5/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m291s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 2.0357e-20 - val_accuracy: 1.0000 - val_loss: 1.9810e-20\n",
      "Epoch 6/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 3.4373e-20 - val_accuracy: 1.0000 - val_loss: 8.8330e-21\n",
      "Epoch 7/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m292s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 2.5425e-19 - val_accuracy: 1.0000 - val_loss: 9.0649e-21\n",
      "Epoch 8/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 135ms/step - accuracy: 1.0000 - loss: 1.8352e-21 - val_accuracy: 1.0000 - val_loss: 7.5179e-21\n",
      "Epoch 9/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m294s\u001b[0m 135ms/step - accuracy: 1.0000 - loss: 9.3682e-17 - val_accuracy: 1.0000 - val_loss: 9.7574e-21\n",
      "Epoch 10/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m293s\u001b[0m 134ms/step - accuracy: 1.0000 - loss: 5.8392e-20 - val_accuracy: 1.0000 - val_loss: 2.6799e-21\n",
      "Epoch 11/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m297s\u001b[0m 136ms/step - accuracy: 1.0000 - loss: 2.4572e-18 - val_accuracy: 1.0000 - val_loss: 8.4010e-21\n",
      "Epoch 12/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 101ms/step - accuracy: 1.0000 - loss: 3.0954e-20 - val_accuracy: 1.0000 - val_loss: 1.5068e-20\n",
      "Epoch 13/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 100ms/step - accuracy: 1.0000 - loss: 2.9955e-21 - val_accuracy: 1.0000 - val_loss: 4.0321e-20\n",
      "Epoch 14/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 96ms/step - accuracy: 1.0000 - loss: 4.4606e-21 - val_accuracy: 1.0000 - val_loss: 1.2475e-20\n",
      "Epoch 15/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 6.3325e-21 - val_accuracy: 1.0000 - val_loss: 5.0499e-20\n",
      "Epoch 16/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 4.6412e-19 - val_accuracy: 1.0000 - val_loss: 3.1186e-21\n",
      "Epoch 17/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 1.1679e-19 - val_accuracy: 1.0000 - val_loss: 1.3413e-21\n",
      "Epoch 18/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 5.4452e-20 - val_accuracy: 1.0000 - val_loss: 3.9165e-21\n",
      "Epoch 19/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 2.9916e-20 - val_accuracy: 1.0000 - val_loss: 1.5188e-20\n",
      "Epoch 20/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 86ms/step - accuracy: 1.0000 - loss: 2.8253e-20 - val_accuracy: 1.0000 - val_loss: 6.5888e-21\n",
      "Epoch 21/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 2.7012e-20 - val_accuracy: 1.0000 - val_loss: 8.3467e-21\n",
      "Epoch 22/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 4.0364e-21 - val_accuracy: 1.0000 - val_loss: 3.2082e-20\n",
      "Epoch 23/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 7.1428e-21 - val_accuracy: 1.0000 - val_loss: 1.6848e-20\n",
      "Epoch 24/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 1.4587e-20 - val_accuracy: 1.0000 - val_loss: 3.5894e-21\n",
      "Epoch 25/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 8.1893e-20 - val_accuracy: 1.0000 - val_loss: 1.6570e-21\n",
      "Epoch 26/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 89ms/step - accuracy: 1.0000 - loss: 6.1088e-21 - val_accuracy: 1.0000 - val_loss: 4.5477e-21\n",
      "Epoch 27/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 88ms/step - accuracy: 1.0000 - loss: 3.5041e-20 - val_accuracy: 1.0000 - val_loss: 7.4936e-21\n",
      "Epoch 28/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 2.6676e-21 - val_accuracy: 1.0000 - val_loss: 4.6949e-20\n",
      "Epoch 29/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 1.7405e-20 - val_accuracy: 1.0000 - val_loss: 1.0746e-20\n",
      "Epoch 30/30\n",
      "\u001b[1m2175/2175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 87ms/step - accuracy: 1.0000 - loss: 8.1809e-21 - val_accuracy: 1.0000 - val_loss: 1.5310e-19\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=30,\n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e825ee3-1b19-4e17-8f18-3b08928857cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m544/544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 1.6921e-19\n",
      "Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "validation_loss, validation_accuracy = model.evaluate(val_generator)\n",
    "print(f'Validation Accuracy: {validation_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5a8a39-9e93-46c0-99ff-b2aaa90fdef8",
   "metadata": {},
   "source": [
    "# Time Restiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afb6a91d-9ac6-4231-b9f8-a6d05345dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_within_time_window():\n",
    "    current_time = datetime.now().time()\n",
    "    start_time = datetime.strptime(\"18:00\", \"%H:%M\").time()\n",
    "    end_time = datetime.strptime(\"22:00\", \"%H:%M\").time()\n",
    "    return start_time <= current_time <= end_time\n",
    "\n",
    "def predict_sign_language(image):\n",
    "    if is_within_time_window():\n",
    "        prediction = model.predict(image)\n",
    "        return np.argmax(prediction, axis=1)\n",
    "    else:\n",
    "        return \"Prediction not allowed outside 6 PM to 10 PM\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "432f6a1b-ef7b-4867-a542-fb6a26a86d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import model_from_json\n",
    "\n",
    "# Load the model architecture from the JSON file\n",
    "with open(\"emotion_detector_model.json\", \"r\") as json_file:\n",
    "    model_json = json_file.read()\n",
    "\n",
    "model = model_from_json(model_json)\n",
    "\n",
    "# Load the weights into the model\n",
    "model.load_weights(\".weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e72fb9b-2f71-417e-bb23-039fab387674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to disk\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model_json = model.to_json()\n",
    "with open(\"Sign_Language_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\".weights.h5\")\n",
    "print(\"Model saved to disk\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5b1b82-8b6c-47d0-aea3-4e88e7fb3cc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52be4bb-a4fb-4a21-b26a-1e9dc2c9e755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae5fe7b-22b5-4e0d-ad57-5f51d4aab336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed266a3-c4b3-48ef-92c9-187f4104999f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b1fbb2-abe8-4388-81c8-66a5c0ce7da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da47f3b8-bf4f-4b0c-bee2-ae3391c80065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f02d6d-71fb-4989-9a46-2e420af82b02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
